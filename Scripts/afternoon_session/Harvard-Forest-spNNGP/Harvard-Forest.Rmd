---
title: "Simulated data and forest canopy height analyses"
output: html_document
---

```{r global_options, echo=FALSE}
knitr::opts_chunk$set(comment = NA, tidy = TRUE)
```

Remove (almost) all objects from the R environment. This will help ensure the program runs without any conflicting objects that may be existing in the workspace.
```{r}
rm(list = ls())
```

# Simulated data analysis

## Make some data
```{r}
rmvn <- function(n, mu=0, V = matrix(1)){
  p <- length(mu)
  if(any(is.na(match(dim(V),p))))
    stop("Dimension problem!")
  D <- chol(V)
  t(matrix(rnorm(n*p), ncol=p)%*%D + rep(mu,rep(n,p)))
}

set.seed(1)
n <- 100
coords <- cbind(runif(n,0,1), runif(n,0,1))

x <- cbind(1, rnorm(n))

B <- as.matrix(c(1,5))

sigma.sq <- 5
tau.sq <- 1
phi <- 3/0.5

D <- as.matrix(dist(coords))
R <- exp(-phi*D)
w <- rmvn(1, rep(0,n), sigma.sq*R)
y <- rnorm(n, x%*%B + w, sqrt(tau.sq))
```

## Fit a Sequential NNGP model
```{r, fig.align="center", message=FALSE}
library(spNNGP)

n.samples <- 500

starting <- list("phi"=phi, "sigma.sq"=5, "tau.sq"=1)

tuning <- list("phi"=0.5, "sigma.sq"=0.5, "tau.sq"=0.5)

priors <- list("phi.Unif"=c(3/1, 3/0.01), "sigma.sq.IG"=c(2, 5), "tau.sq.IG"=c(2, 1))

cov.model <- "exponential"

m.s <- spNNGP(y~x-1, coords=coords, starting=starting, method="latent", n.neighbors=5, tuning=tuning, priors=priors, cov.model=cov.model, n.samples=n.samples, return.neighbors = TRUE, n.omp.threads=2)

round(summary(m.s$p.beta.samples)$quantiles[,c(3,1,5)],2)
round(summary(m.s$p.theta.samples)$quantiles[,c(3,1,5)],2)
plot(w, apply(m.s$p.w.samples, 1, median),  xlab="True w", ylab="Posterior median w")
```
# Harvard Forest canopy height analysis

Here we consider forest canopy height (m) data measured using the NASA Goddard's LiDAR Hyperspectral and Thermal (G-LiHT) Airborne Imager over a subset of Harvard Forest Simes Tract, MA, collected in Summer 2012. This is a sampling LiDAR system that only records strips of canopy height across the landscape. We would like to use the Harvard Forest data to assess if the current density of LiDAR measurements can be reduced, which would allow for wider strips to be collected. Ultimately, interest is in creating wall-to-wall maps of forest canopy height with associated uncertainty.

Let's load the necessary packages and canopy height data which are part of the `spNNGP` package. Here too, we subset the data and divide it into a model and testing set.
```{r, message=FALSE}
library(geoR)
library(raster)
library(leaflet)

CHM <- raster(paste0("HARV/CHM/HARV_chmCrop.tif"))

CHM <- as.data.frame(CHM, xy=TRUE)
CHM <- CHM[(CHM[,3]>0),]

row.has.na <- apply(CHM, 1, function(x){any(is.na(x))})
CHM <- CHM[(!row.has.na),]

set.seed(1)
mod <- sample(1:nrow(CHM), 25000)
ho <- sample((1:nrow(CHM))[-mod], 10000)

CHM.mod <- CHM[mod,]
CHM.ho <- CHM[ho,]
```

Let's again start with a `leaflet` basemap then overlay the canopy height data. Recall, `leaflet` maps expect data to be in geographic coordinate system (i.e., longitude and latitude), so we first need reproject the CHM data (just for visualization purposes, we'll fit the model using the projected coordinates).

```{r}
chm.r <- rasterFromXYZ(CHM)
proj4string(chm.r) <- "+proj=utm +zone=18 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"
chm.r.ll <- projectRaster(chm.r, crs="+proj=longlat +datum=WGS84")

pal <- colorNumeric(rev(terrain.colors(50)), domain = values(chm.r.ll), na.color = "transparent")

base.map <- leaflet(width="100%") %>%
    addProviderTiles("Esri.WorldImagery", group="Satellite") %>%
    addProviderTiles("Esri.WorldShadedRelief", group="Terrain")

base.map %>%
    addRasterImage(chm.r.ll, colors = pal, opacity = 1, group="Canopy height") %>%
    addLegend("bottomright", pal = pal, values = values(chm.r.ll), opacity = 1, title = "<center>Canopy height (m)</center>") %>%
    addLayersControl(
        baseGroup = c("Satellite", "Terrain"),
        overlayGroups = c("Canopy height"),
        options = layersControlOptions(collapsed = FALSE)
    )
```

Let's try and fit a variogram to the data to get a sense of the spatial structure. These `variog` function calculates the $n\times n$ Euclidean distance matrix to construct the empirical variogram. When $n$ is large this will you will likely run out of memory, so you might need to consider only a subset of your data.

```{r, fig.align="center"}
sub <- 1:10000

#note, max intersite distance is ~1.5km
v <- variog(coords=CHM.mod[sub,1:2], data=CHM.mod[sub,3], uvec=(seq(0, 500, length=30))) 

plot(v, xlab="Distance (m)")
```

Now let's fit some spatial regression models using NNGP random effects.
```{r}
n.samples <- 1000

starting <- list("phi"=3/50, "sigma.sq"=15, "tau.sq"=2.5)

tuning <- list("phi"=0.05, "sigma.sq"=0.01, "tau.sq"=0.01)

priors <- list("phi.Unif"=c(3/1000, 3/10), "sigma.sq.IG"=c(2, 10), "tau.sq.IG"=c(2, 5))

cov.model <- "exponential"

##Response model 
m.r <- spNNGP(CHM.mod[,3] ~ 1, coords=CHM.mod[,1:2], starting=starting, method="response", n.neighbors=10,
              tuning=tuning, priors=priors, cov.model=cov.model,
              n.samples=n.samples, n.omp.threads=2, n.report=100)

round(summary(m.r$p.beta.samples)$quantiles[c(3,1,5)],2)
round(summary(m.r$p.theta.samples)$quantiles[,c(3,1,5)],3)

plot(m.r$p.beta.samples)
plot(m.r$p.theta.samples)

m.r$run.time

```

Now prediction for the holdout set. 

```{r, fig.align="center"}
burn.in <- floor(0.5*n.samples)

X.pred <- as.matrix(rep(1,nrow(CHM.ho)), ncol=1, byrow=T)

pred.coords.x <- as.numeric(CHM.ho$x)
pred.coords.y <- as.numeric(CHM.ho$y)
pred.coords <- cbind(pred.coords.x, pred.coords.y)

p.r <- predict(m.r, X.0 = X.pred, coords.0 = pred.coords, sub.sample = list(start=burn.in, end=n.samples, thin=2), n.omp.threads=2)

y.hat.r <- apply(p.r$p.y.0, 1, mean)

plot(CHM.ho[,3], y.hat.r, main="Response NNGP model", xlab="True canopy height", ylab="Posterior predictive distribution mean")
```


